import numpy as np
import math
import sys
from numpy.random import randn


def initliaize_parameters(layer_dims):
    """
    :param layer_dims: array of dimensions for each layer in the network
    :return: a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL)
    """
    init_dict = dict()
    init_dict[0] = layer_dims[0]
    for idx in range(1, len(layer_dims)):  # Starts from 1 - layer 0 is the input
        cur_dim = layer_dims[idx]
        init_dict[idx] = (randn(cur_dim, layer_dims[idx - 1]) * np.sqrt(2/layer_dims[idx - 1]) , # [4X3] W in the size of (layer_dim, prev_layer_dim)
                          np.zeros((cur_dim, 1)))  # [4X1] b in the size (layer_dim, 1)
    return init_dict


def linear_forward(A, W, B):
    """
    :param A: the activations of the previous layer
    :param W: the weight matrix of the current layer
    (of shape [size of current layer, size of previous layer])
    :param B: the bias vector of the current layer (of shape [size of current layer, 1])
    :return: Z – the linear component of the activation function
    linear_cache – a dictionary containing (A, W, B)
    """
    results = W.dot(A) # (W*A)
    Z = results + B
    linear_cache = {'A':A,'W': W,'B': B}
    return Z, linear_cache



def sigmoid(Z):
    """
    :param Z: the linear component of the activation function
    :return: A - The activations of the layer
    activation_cache – returns Z, which will be useful for the backpropagation
    """
    A = 1. / (1. + np.exp(-Z))
    return A, Z



def relu(Z):
    """
    :param Z: the linear component of the activation function
    :return: A - The activations of the layer
    activation_cache – returns Z, which will be useful for the backpropagation
    """

    A = np.maximum(0, Z)  # Return the value just calculated
    return A, Z



def softmax(Z):
    '''
    :param Z: the linear component of the activation function
    :return: A – the activations of the layer
    activation_cache – returns Z, which will be useful for the backpropagation
    '''
    eps=sys.float_info.epsilon
    expA = np.nan_to_num(np.exp(Z)) + eps
    A = expA / (expA.sum(axis=0, keepdims=True)+eps)
    return A , Z


def linear_activation_forward(A_prev, W, B, activation='relu'):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    :param A_prev: activations of the previous layer
    :param W: the weights matrix of the current layer
    :param B: the bias vector of the current layer
    :param activation: the activation function to be used (a string, either “sigmoid” or “relu”)
    :return: A – the activations of the current layer
    cache – a joint dictionary containing both linear_cache and activation_cache
    """
    Z, linear_cache = linear_forward(A_prev, W, B)
    if activation == 'softmax':
        A, activation_cache = softmax(Z)
    else:
        A, activation_cache = relu(Z)
    return A, {'linear_cache':linear_cache, 'activation_cache':activation_cache}



def L_model_forward(X, parameters, use_batchnorm=False,dropout_keep_prob=1):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation
    :param X: the data, numpy array of shape (input size, number of examples)
    :param parameters: the initialized W and b parameters of each layer
    :param use_batchnorm: a boolean flag used to determine whether to apply batchnorm after activation
    :return:
    AL – the last post-activation value
    caches – a list of all the cache objects generated by the linear_forward function
    """
    activation = 'relu'
    caches = []
    A = X
    for layer, params in parameters.items():
        if layer == 0:
            if X.shape[0] != params:
                sys.exit("shape of input layer not equal to input array")
            else:
                continue
        if layer == len(parameters) - 1:
            activation = "softmax"
        A, cache = linear_activation_forward(A, params[0], params[1], activation)
        if use_batchnorm and activation != "softmax":
            A = apply_batchnorm(A)
        
        #******* dropout *******
        if dropout_keep_prob!=1 and activation != "softmax":
            A, D = dropout(A,dropout_keep_prob)
            cache['D']=D
        #******* dropout *******
        caches.append(cache)
    return A, caches


def compute_cost(AL, Y):
    """
    Implement the cost function defined by equation
    :param AL: probability vector corresponding to your label predictions, shape (1, number of examples)
    :param Y: the labels vector (i.e. the ground truth)
    :return: cost – the cross-entropy cost
    """
    eps=sys.float_info.epsilon
    m = AL.shape[0]
    Y = Y.T
    cost = np.sum(np.multiply(Y,np.log(AL+eps)) + np.multiply((1 - Y),np.log(1 - AL+eps)))
    cost = -(cost / m)
    return cost


def apply_batchnorm(A):
    """
    performs batchnorm on the received activation values of a given layer
    :param A: the activation values of a given layer
    :return: NA - the normalized activation values, based on the formula learned in class
    """
    eps=sys.float_info.epsilon
    
    A=A.T
    meanA = A.mean(axis=0)
    varA = A.var(axis=0)
    NA = (A-meanA)/np.sqrt(varA+eps)

    return NA.T


# =============================================================================
#  Bonus - dropout   
# =============================================================================

def dropout(A,keep_prob):
    """
    performs dropout on the received activation values of a given layer
    param: A the activation values of a given layer
    return: DA - the A after dropout
    """
    #create drop out vectors
    D = np.random.rand(A.shape[0], A.shape[1])
    D = D < keep_prob
    DA = np.multiply(A, D)/keep_prob   
    return DA , D
    
 

   

